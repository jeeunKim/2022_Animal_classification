{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb68a4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 5 classes.\n",
      "Found 1000 images belonging to 5 classes.\n",
      "Epoch 1/30\n",
      "200/200 - 103s - loss: 1.2936 - accuracy: 0.4487 - val_loss: 0.8798 - val_accuracy: 0.6360 - 103s/epoch - 514ms/step\n",
      "Epoch 2/30\n",
      "200/200 - 90s - loss: 0.8380 - accuracy: 0.6672 - val_loss: 0.7106 - val_accuracy: 0.7400 - 90s/epoch - 452ms/step\n",
      "Epoch 3/30\n",
      "200/200 - 90s - loss: 0.6382 - accuracy: 0.7525 - val_loss: 0.7338 - val_accuracy: 0.7530 - 90s/epoch - 448ms/step\n",
      "Epoch 4/30\n",
      "200/200 - 92s - loss: 0.5042 - accuracy: 0.8048 - val_loss: 0.5918 - val_accuracy: 0.7890 - 92s/epoch - 459ms/step\n",
      "Epoch 5/30\n",
      "200/200 - 103s - loss: 0.3928 - accuracy: 0.8462 - val_loss: 0.7017 - val_accuracy: 0.7760 - 103s/epoch - 514ms/step\n",
      "Epoch 6/30\n",
      "200/200 - 104s - loss: 0.3070 - accuracy: 0.8838 - val_loss: 0.6653 - val_accuracy: 0.7940 - 104s/epoch - 518ms/step\n",
      "Epoch 7/30\n",
      "200/200 - 115s - loss: 0.2229 - accuracy: 0.9133 - val_loss: 0.7139 - val_accuracy: 0.7790 - 115s/epoch - 573ms/step\n",
      "Epoch 8/30\n",
      "200/200 - 108s - loss: 0.1777 - accuracy: 0.9317 - val_loss: 0.9017 - val_accuracy: 0.7860 - 108s/epoch - 540ms/step\n",
      "Epoch 9/30\n",
      "200/200 - 96s - loss: 0.1495 - accuracy: 0.9467 - val_loss: 0.8558 - val_accuracy: 0.7750 - 96s/epoch - 482ms/step\n",
      "Epoch 10/30\n",
      "200/200 - 98s - loss: 0.1426 - accuracy: 0.9495 - val_loss: 0.8516 - val_accuracy: 0.7890 - 98s/epoch - 488ms/step\n",
      "Epoch 11/30\n",
      "200/200 - 99s - loss: 0.0774 - accuracy: 0.9750 - val_loss: 1.0417 - val_accuracy: 0.7990 - 99s/epoch - 497ms/step\n",
      "Epoch 12/30\n",
      "200/200 - 97s - loss: 0.0726 - accuracy: 0.9762 - val_loss: 1.2172 - val_accuracy: 0.7740 - 97s/epoch - 486ms/step\n",
      "Epoch 13/30\n",
      "200/200 - 99s - loss: 0.0731 - accuracy: 0.9762 - val_loss: 1.0607 - val_accuracy: 0.7930 - 99s/epoch - 494ms/step\n",
      "Epoch 14/30\n",
      "200/200 - 94s - loss: 0.0545 - accuracy: 0.9825 - val_loss: 1.1519 - val_accuracy: 0.7940 - 94s/epoch - 472ms/step\n",
      "Epoch 15/30\n",
      "200/200 - 99s - loss: 0.0739 - accuracy: 0.9758 - val_loss: 1.1027 - val_accuracy: 0.7810 - 99s/epoch - 493ms/step\n",
      "Epoch 16/30\n",
      "200/200 - 100s - loss: 0.0710 - accuracy: 0.9772 - val_loss: 1.0801 - val_accuracy: 0.7920 - 100s/epoch - 498ms/step\n",
      "Epoch 17/30\n",
      "200/200 - 101s - loss: 0.0593 - accuracy: 0.9797 - val_loss: 1.0334 - val_accuracy: 0.8050 - 101s/epoch - 506ms/step\n",
      "Epoch 18/30\n",
      "200/200 - 104s - loss: 0.0596 - accuracy: 0.9827 - val_loss: 1.2856 - val_accuracy: 0.7730 - 104s/epoch - 521ms/step\n",
      "Epoch 19/30\n",
      "200/200 - 95s - loss: 0.0296 - accuracy: 0.9910 - val_loss: 1.3564 - val_accuracy: 0.7830 - 95s/epoch - 473ms/step\n",
      "Epoch 20/30\n",
      "200/200 - 100s - loss: 0.0441 - accuracy: 0.9875 - val_loss: 1.4779 - val_accuracy: 0.7760 - 100s/epoch - 501ms/step\n",
      "Epoch 21/30\n",
      "200/200 - 99s - loss: 0.0470 - accuracy: 0.9868 - val_loss: 1.3288 - val_accuracy: 0.7810 - 99s/epoch - 496ms/step\n",
      "Epoch 22/30\n",
      "200/200 - 95s - loss: 0.0640 - accuracy: 0.9780 - val_loss: 1.1843 - val_accuracy: 0.8010 - 95s/epoch - 475ms/step\n",
      "Epoch 23/30\n",
      "200/200 - 96s - loss: 0.0308 - accuracy: 0.9900 - val_loss: 1.7320 - val_accuracy: 0.7670 - 96s/epoch - 479ms/step\n",
      "Epoch 24/30\n",
      "200/200 - 99s - loss: 0.0433 - accuracy: 0.9868 - val_loss: 1.3032 - val_accuracy: 0.7760 - 99s/epoch - 495ms/step\n",
      "Epoch 25/30\n",
      "200/200 - 94s - loss: 0.0261 - accuracy: 0.9927 - val_loss: 1.7805 - val_accuracy: 0.7500 - 94s/epoch - 470ms/step\n",
      "Epoch 26/30\n",
      "200/200 - 97s - loss: 0.0285 - accuracy: 0.9900 - val_loss: 1.7877 - val_accuracy: 0.7570 - 97s/epoch - 483ms/step\n",
      "Epoch 27/30\n",
      "200/200 - 97s - loss: 0.0960 - accuracy: 0.9760 - val_loss: 1.1906 - val_accuracy: 0.7670 - 97s/epoch - 487ms/step\n",
      "Epoch 28/30\n",
      "200/200 - 99s - loss: 0.0329 - accuracy: 0.9887 - val_loss: 1.4672 - val_accuracy: 0.7630 - 99s/epoch - 494ms/step\n",
      "Epoch 29/30\n",
      "200/200 - 97s - loss: 0.0393 - accuracy: 0.9868 - val_loss: 1.5461 - val_accuracy: 0.7940 - 97s/epoch - 485ms/step\n",
      "Epoch 30/30\n",
      "200/200 - 91s - loss: 0.0293 - accuracy: 0.9918 - val_loss: 1.4509 - val_accuracy: 0.7940 - 91s/epoch - 454ms/step\n",
      "정확률은  79.40000295639038\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "base_dir = '/Users/인공지능'\n",
    "\n",
    "train_dir = os.path.join(base_dir,'train')\n",
    "validation_dir = os.path.join(base_dir,'validation')\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size = (150, 150),\n",
    "    batch_size = 20,\n",
    "    class_mode = \"categorical\"\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size = (150, 150),\n",
    "    batch_size = 20,\n",
    "    class_mode = \"categorical\"\n",
    ")\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Conv2D(32,(3, 3), input_shape=(150, 150, 3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(64,(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(128,(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(256,(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(units=512, activation=\"relu\"))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(units=5, activation=\"softmax\"))\n",
    "\n",
    "model1.compile(loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])\n",
    "hist = model1.fit(train_generator,\n",
    "                    validation_data=test_generator,\n",
    "                    batch_size=100,\n",
    "                    epochs=30,\n",
    "                    verbose=2)\n",
    "\n",
    "res = model1.evaluate(test_generator,verbose = 0)\n",
    "print(\"정확률은 \",res[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "363d500b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 5 classes.\n",
      "Found 1000 images belonging to 1 classes.\n",
      "Epoch 1/30\n",
      "200/200 - 97s - loss: 1.3021 - accuracy: 0.4478 - val_loss: 12.5547 - val_accuracy: 0.2220 - 97s/epoch - 484ms/step\n",
      "Epoch 2/30\n",
      "200/200 - 108s - loss: 0.8459 - accuracy: 0.6662 - val_loss: 17.5152 - val_accuracy: 0.2090 - 108s/epoch - 540ms/step\n",
      "Epoch 3/30\n",
      "200/200 - 106s - loss: 0.6533 - accuracy: 0.7470 - val_loss: 18.8839 - val_accuracy: 0.2470 - 106s/epoch - 528ms/step\n",
      "Epoch 4/30\n",
      "200/200 - 97s - loss: 0.5221 - accuracy: 0.8012 - val_loss: 29.2348 - val_accuracy: 0.2050 - 97s/epoch - 483ms/step\n",
      "Epoch 5/30\n",
      "200/200 - 95s - loss: 0.4039 - accuracy: 0.8450 - val_loss: 28.3420 - val_accuracy: 0.2260 - 95s/epoch - 477ms/step\n",
      "Epoch 6/30\n",
      "200/200 - 97s - loss: 0.3167 - accuracy: 0.8773 - val_loss: 41.0302 - val_accuracy: 0.2360 - 97s/epoch - 486ms/step\n",
      "Epoch 7/30\n",
      "200/200 - 96s - loss: 0.2372 - accuracy: 0.9115 - val_loss: 44.9493 - val_accuracy: 0.2020 - 96s/epoch - 481ms/step\n",
      "Epoch 8/30\n",
      "200/200 - 94s - loss: 0.1507 - accuracy: 0.9467 - val_loss: 48.0999 - val_accuracy: 0.2100 - 94s/epoch - 471ms/step\n",
      "Epoch 9/30\n",
      "200/200 - 93s - loss: 0.1082 - accuracy: 0.9643 - val_loss: 56.8167 - val_accuracy: 0.2190 - 93s/epoch - 464ms/step\n",
      "Epoch 10/30\n",
      "200/200 - 99s - loss: 0.1187 - accuracy: 0.9572 - val_loss: 58.7499 - val_accuracy: 0.2160 - 99s/epoch - 496ms/step\n",
      "Epoch 11/30\n",
      "200/200 - 101s - loss: 0.0951 - accuracy: 0.9675 - val_loss: 69.5206 - val_accuracy: 0.2300 - 101s/epoch - 505ms/step\n",
      "Epoch 12/30\n",
      "200/200 - 101s - loss: 0.0708 - accuracy: 0.9747 - val_loss: 66.9521 - val_accuracy: 0.2330 - 101s/epoch - 503ms/step\n",
      "Epoch 13/30\n",
      "200/200 - 112s - loss: 0.0938 - accuracy: 0.9665 - val_loss: 73.1490 - val_accuracy: 0.2390 - 112s/epoch - 560ms/step\n",
      "Epoch 14/30\n",
      "200/200 - 92s - loss: 0.0762 - accuracy: 0.9772 - val_loss: 71.0322 - val_accuracy: 0.2150 - 92s/epoch - 459ms/step\n",
      "Epoch 15/30\n",
      "200/200 - 99s - loss: 0.0653 - accuracy: 0.9787 - val_loss: 67.4631 - val_accuracy: 0.2030 - 99s/epoch - 497ms/step\n",
      "Epoch 16/30\n",
      "200/200 - 95s - loss: 0.0478 - accuracy: 0.9852 - val_loss: 89.3513 - val_accuracy: 0.2050 - 95s/epoch - 477ms/step\n",
      "Epoch 17/30\n",
      "200/200 - 95s - loss: 0.0357 - accuracy: 0.9912 - val_loss: 86.3884 - val_accuracy: 0.2090 - 95s/epoch - 475ms/step\n",
      "Epoch 18/30\n",
      "200/200 - 95s - loss: 0.0448 - accuracy: 0.9858 - val_loss: 84.9335 - val_accuracy: 0.1860 - 95s/epoch - 473ms/step\n",
      "Epoch 19/30\n",
      "200/200 - 99s - loss: 0.0507 - accuracy: 0.9855 - val_loss: 87.0017 - val_accuracy: 0.2120 - 99s/epoch - 494ms/step\n",
      "Epoch 20/30\n",
      "200/200 - 91s - loss: 0.0563 - accuracy: 0.9797 - val_loss: 87.0207 - val_accuracy: 0.2140 - 91s/epoch - 455ms/step\n",
      "Epoch 21/30\n",
      "200/200 - 92s - loss: 0.0444 - accuracy: 0.9868 - val_loss: 91.3393 - val_accuracy: 0.2150 - 92s/epoch - 461ms/step\n",
      "Epoch 22/30\n",
      "200/200 - 92s - loss: 0.0473 - accuracy: 0.9840 - val_loss: 86.6214 - val_accuracy: 0.2230 - 92s/epoch - 461ms/step\n",
      "Epoch 23/30\n",
      "200/200 - 90s - loss: 0.0340 - accuracy: 0.9872 - val_loss: 91.4228 - val_accuracy: 0.2280 - 90s/epoch - 451ms/step\n",
      "Epoch 24/30\n",
      "200/200 - 93s - loss: 0.0558 - accuracy: 0.9845 - val_loss: 84.1311 - val_accuracy: 0.2070 - 93s/epoch - 464ms/step\n",
      "Epoch 25/30\n",
      "200/200 - 93s - loss: 0.0355 - accuracy: 0.9902 - val_loss: 95.4756 - val_accuracy: 0.2130 - 93s/epoch - 467ms/step\n",
      "Epoch 26/30\n",
      "200/200 - 90s - loss: 0.0356 - accuracy: 0.9898 - val_loss: 103.5763 - val_accuracy: 0.2130 - 90s/epoch - 449ms/step\n",
      "Epoch 27/30\n",
      "200/200 - 91s - loss: 0.0315 - accuracy: 0.9902 - val_loss: 105.4490 - val_accuracy: 0.2140 - 91s/epoch - 454ms/step\n",
      "Epoch 28/30\n",
      "200/200 - 90s - loss: 0.0229 - accuracy: 0.9923 - val_loss: 116.5009 - val_accuracy: 0.2230 - 90s/epoch - 448ms/step\n",
      "Epoch 29/30\n",
      "200/200 - 92s - loss: 0.0448 - accuracy: 0.9865 - val_loss: 97.3785 - val_accuracy: 0.2190 - 92s/epoch - 459ms/step\n",
      "Epoch 30/30\n",
      "200/200 - 92s - loss: 0.0396 - accuracy: 0.9887 - val_loss: 100.5306 - val_accuracy: 0.2290 - 92s/epoch - 462ms/step\n",
      "정확률은  22.90000021457672\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "base_dir = '/Users/인공지능'\n",
    "\n",
    "train_dir = os.path.join(base_dir,'train')\n",
    "validation_dir = os.path.join(base_dir,'validation')\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size = (150, 150),\n",
    "    batch_size = 20,\n",
    "    class_mode = \"categorical\"\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size = (150, 150),\n",
    "    batch_size = 20,\n",
    "    class_mode = \"categorical\"\n",
    ")\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Conv2D(32,(3, 3), input_shape=(150, 150, 3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(64,(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(128,(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(256,(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(units=512, activation=\"relu\"))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(units=5, activation=\"softmax\"))\n",
    "\n",
    "model1.compile(loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])\n",
    "hist = model1.fit(train_generator,\n",
    "                    validation_data=test_generator,\n",
    "                    batch_size=100,\n",
    "                    epochs=30,\n",
    "                    verbose=2)\n",
    "\n",
    "res = model1.evaluate(test_generator,verbose = 0)\n",
    "print(\"정확률은 \",res[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eff96d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (1.20.3)\n",
      "Requirement already satisfied: image in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (1.5.33)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (from image) (8.4.0)\n",
      "Requirement already satisfied: django in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (from image) (4.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\ygvhn\\appdata\\roaming\\python\\python39\\site-packages (from image) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: asgiref<4,>=3.4.1 in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (from django->image) (3.5.2)\n",
      "Requirement already satisfied: tzdata in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (from django->image) (2022.1)\n",
      "Requirement already satisfied: sqlparse>=0.2.2 in c:\\users\\ygvhn\\anaconda3\\lib\\site-packages (from django->image) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install keras pandas numpy image matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5364c96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 validated image filenames belonging to 5 classes.\n",
      "Found 1000 validated image filenames belonging to 5 classes.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ygvhn\\AppData\\Local\\Temp/ipykernel_27704/2622245990.py:100: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 71s 351ms/step - loss: 1.3961 - accuracy: 0.5715 - val_loss: 8.2213 - val_accuracy: 0.1920\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 66s 331ms/step - loss: 0.8739 - accuracy: 0.7050 - val_loss: 5.7371 - val_accuracy: 0.2000\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 66s 332ms/step - loss: 0.6456 - accuracy: 0.7692 - val_loss: 1.1695 - val_accuracy: 0.6490\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 70s 348ms/step - loss: 0.5388 - accuracy: 0.8073 - val_loss: 1.4530 - val_accuracy: 0.5370\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 71s 356ms/step - loss: 0.4251 - accuracy: 0.8428 - val_loss: 0.5708 - val_accuracy: 0.7860\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 72s 358ms/step - loss: 0.3454 - accuracy: 0.8695 - val_loss: 0.8731 - val_accuracy: 0.7030\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 82s 410ms/step - loss: 0.2898 - accuracy: 0.8945 - val_loss: 0.5912 - val_accuracy: 0.7890\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 71s 354ms/step - loss: 0.2299 - accuracy: 0.9183 - val_loss: 0.8392 - val_accuracy: 0.7580\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 67s 335ms/step - loss: 0.2173 - accuracy: 0.9208 - val_loss: 0.6983 - val_accuracy: 0.7920\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 68s 341ms/step - loss: 0.1697 - accuracy: 0.9390 - val_loss: 0.8937 - val_accuracy: 0.7530\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 69s 343ms/step - loss: 0.1455 - accuracy: 0.9490 - val_loss: 0.7697 - val_accuracy: 0.7910\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 69s 346ms/step - loss: 0.1329 - accuracy: 0.9553 - val_loss: 1.0670 - val_accuracy: 0.7680\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.1249 - accuracy: 0.9572 - val_loss: 1.3672 - val_accuracy: 0.7260\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 71s 354ms/step - loss: 0.1061 - accuracy: 0.9610 - val_loss: 0.6375 - val_accuracy: 0.8240\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 71s 354ms/step - loss: 0.0972 - accuracy: 0.9707 - val_loss: 0.9049 - val_accuracy: 0.7810\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 80s 400ms/step - loss: 0.1073 - accuracy: 0.9632 - val_loss: 0.8748 - val_accuracy: 0.7830\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 73s 363ms/step - loss: 0.0864 - accuracy: 0.9712 - val_loss: 1.2995 - val_accuracy: 0.7820\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 70s 349ms/step - loss: 0.0745 - accuracy: 0.9753 - val_loss: 0.7448 - val_accuracy: 0.8040\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 70s 351ms/step - loss: 0.0682 - accuracy: 0.9808 - val_loss: 0.9276 - val_accuracy: 0.7750\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 69s 347ms/step - loss: 0.0841 - accuracy: 0.9720 - val_loss: 1.5986 - val_accuracy: 0.7460\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 70s 351ms/step - loss: 0.0672 - accuracy: 0.9758 - val_loss: 1.1194 - val_accuracy: 0.7870\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 72s 359ms/step - loss: 0.0749 - accuracy: 0.9760 - val_loss: 1.1439 - val_accuracy: 0.7840\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 76s 381ms/step - loss: 0.0820 - accuracy: 0.9695 - val_loss: 1.1684 - val_accuracy: 0.7920\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 71s 356ms/step - loss: 0.0865 - accuracy: 0.9720 - val_loss: 1.3979 - val_accuracy: 0.7860\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 72s 360ms/step - loss: 0.0874 - accuracy: 0.9707 - val_loss: 0.9278 - val_accuracy: 0.7870\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 74s 368ms/step - loss: 0.0629 - accuracy: 0.9793 - val_loss: 0.7949 - val_accuracy: 0.8190\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 68s 338ms/step - loss: 0.0695 - accuracy: 0.9750 - val_loss: 1.5407 - val_accuracy: 0.7690\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 74s 373ms/step - loss: 0.0840 - accuracy: 0.9758 - val_loss: 1.0652 - val_accuracy: 0.8000\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 75s 373ms/step - loss: 0.0720 - accuracy: 0.9743 - val_loss: 1.3303 - val_accuracy: 0.7690\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 69s 344ms/step - loss: 0.0699 - accuracy: 0.9790 - val_loss: 0.9319 - val_accuracy: 0.7970\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "FAST_RUN = False\n",
    "IMAGE_WIDTH = 128\n",
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_SIZE = (128,128)\n",
    "IMAGE_CHANNELS=3\n",
    "\n",
    "filenames = os.listdir(\"/Users/인공지능/trainn/train\")\n",
    "categories = []\n",
    "for filename in  filenames:\n",
    "    category = filename.split(' ')[0]\n",
    "    if category == 'cat':\n",
    "        categories.append(0)\n",
    "    elif category == 'dog':\n",
    "        categories.append(1)\n",
    "    elif category == 'elephant':\n",
    "        categories.append(2)\n",
    "    elif category == 'horse':\n",
    "        categories.append(3)\n",
    "    else:\n",
    "        categories.append(4)\n",
    "        \n",
    "df = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'category': categories\n",
    "})\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation = 'relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128,(3,3), activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5,activation ='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "df[\"category\"]= df[\"category\"].replace({0:'cat', 1: 'dog', 2: 'elephant', 3: 'horse',4:'lion'})\n",
    "\n",
    "train_df, validate_df = train_test_split(df, test_size=0.20, random_state=42)\n",
    "train_df = train_df.reset_index(drop =True)\n",
    "validate_df = validate_df.reset_index(drop =True)\n",
    "\n",
    "total_train = train_df.shape[0]\n",
    "total_validate = validate_df.shape[0]\n",
    "batch_size = 20\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    ")\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    \"/Users/인공지능/trainn/train\",\n",
    "    x_col='filename',\n",
    "    y_col='category',\n",
    "    target_size= IMAGE_SIZE,\n",
    "    class_mode = \"categorical\",\n",
    "    batch_size = batch_size\n",
    ")\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    ")\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    validate_df,\n",
    "    \"/Users/인공지능/trainn/train\",\n",
    "    x_col='filename',\n",
    "    y_col='category',\n",
    "    target_size= IMAGE_SIZE,\n",
    "    class_mode = \"categorical\",\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "epochs=30\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps =total_validate//batch_size,\n",
    "    steps_per_epoch=total_train//batch_size,\n",
    ")\n",
    "model.save_weights(\"model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cfca9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             filename category\n",
      "0      lion (302).jpg     lion\n",
      "1      lion (707).jpg     lion\n",
      "2       cat (819).jpg      cat\n",
      "3     horse (702).jpg    horse\n",
      "4      lion (272).jpg     lion\n",
      "...               ...      ...\n",
      "3995   lion (482).jpg     lion\n",
      "3996    cat (518).jpg      cat\n",
      "3997  horse (181).jpg    horse\n",
      "3998  horse (794).jpg    horse\n",
      "3999    cat (873).jpg      cat\n",
      "\n",
      "[4000 rows x 2 columns]\n",
      "               filename  category\n",
      "0          dog (55).jpg       dog\n",
      "1    elephant (626).jpg  elephant\n",
      "2    elephant (687).jpg  elephant\n",
      "3         dog (148).jpg       dog\n",
      "4         cat (733).jpg       cat\n",
      "..                  ...       ...\n",
      "995      lion (739).jpg      lion\n",
      "996  elephant (380).jpg  elephant\n",
      "997     horse (291).jpg     horse\n",
      "998  elephant (758).jpg  elephant\n",
      "999       dog (932).jpg       dog\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df)\n",
    "print(validate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f127eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filenames = os.listdir(\"/Users/인공지능/trainn/test\")\n",
    "test_df = pd.DataFrame({\n",
    "    'filename': test_filenames\n",
    "})\n",
    "nb_samples = test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3cadb55c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_gen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    \"/Users/인공지능/trainn/test\",\n",
    "    x_col='filename',\n",
    "    y_col=None,\n",
    "    class_mode = None,\n",
    "    target_size = IMAGE_SIZE,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "000cb69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ygvhn\\AppData\\Local\\Temp/ipykernel_27704/2670138283.py:1: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  predict = model.predict_generator(test_generator, steps = np.ceil(nb_samples/batch_size))\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict_generator(test_generator, steps = np.ceil(nb_samples/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "909b2306",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['category']=np.argmax(predict, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8bfa4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['category'] = test_df['category'].replace({'cat':0, 'dog':1, 'elephant':2, 'horse':3, 'lion':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d51b8970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확률은  79.6999990940094\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(validation_generator,verbose = 0)\n",
    "print(\"정확률은 \",res[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a59d018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df['category'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b67b57fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               filename  category\n",
      "0        test_img_0.jpg         3\n",
      "1        test_img_1.jpg         1\n",
      "2       test_img_10.jpg         1\n",
      "3      test_img_100.jpg         1\n",
      "4     test_img_1000.jpg         2\n",
      "...                 ...       ...\n",
      "4995   test_img_995.jpg         1\n",
      "4996   test_img_996.jpg         4\n",
      "4997   test_img_997.jpg         3\n",
      "4998   test_img_998.jpg         1\n",
      "4999   test_img_999.jpg         4\n",
      "\n",
      "[5000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d75011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('ac.csv', mode='a',header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4697dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
